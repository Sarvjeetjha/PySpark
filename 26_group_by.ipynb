{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-27E2FOUT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>groupBy()</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eae257e350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('groupBy()').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method groupBy in module pyspark.sql.dataframe:\n",
      "\n",
      "groupBy(*cols: 'ColumnOrName') -> 'GroupedData' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Groups the :class:`DataFrame` using the specified columns,\n",
      "    so we can run aggregation on them. See :class:`GroupedData`\n",
      "    for all the available aggregate functions.\n",
      "    \n",
      "    :func:`groupby` is an alias for :func:`groupBy`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : list, str or :class:`Column`\n",
      "        columns to group by.\n",
      "        Each element should be a column name (string) or an expression (:class:`Column`)\n",
      "        or list of them.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`GroupedData`\n",
      "        Grouped data by given columns.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([\n",
      "    ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "    \n",
      "    Empty grouping columns triggers a global aggregation.\n",
      "    \n",
      "    >>> df.groupBy().avg().show()\n",
      "    +--------+\n",
      "    |avg(age)|\n",
      "    +--------+\n",
      "    |    2.75|\n",
      "    +--------+\n",
      "    \n",
      "    Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
      "    \n",
      "    >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
      "    +-----+--------+\n",
      "    | name|sum(age)|\n",
      "    +-----+--------+\n",
      "    |Alice|       2|\n",
      "    |  Bob|       9|\n",
      "    +-----+--------+\n",
      "    \n",
      "    Group-by 'name', and calculate maximum values.\n",
      "    \n",
      "    >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
      "    +-----+--------+\n",
      "    | name|max(age)|\n",
      "    +-----+--------+\n",
      "    |Alice|       2|\n",
      "    |  Bob|       5|\n",
      "    +-----+--------+\n",
      "    \n",
      "    Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
      "    \n",
      "    >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
      "    +-----+---+-----+\n",
      "    | name|age|count|\n",
      "    +-----+---+-----+\n",
      "    |Alice|  2|    1|\n",
      "    |  Bob|  2|    2|\n",
      "    |  Bob|  5|    1|\n",
      "    +-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.groupBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark groupBy on DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- sum(salary): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do the groupBy() on department column of DataFrame and then find the sum of salary for each department using sum() function.\n",
    "df2=df.groupBy('department').sum('salary')\n",
    "df2.printSchema()\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate the number of employees in each department using.\n",
    "df3=df.groupBy('department').count()\n",
    "df3.printSchema()\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- min(salary): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the minimum salary of each department using min()\n",
    "df4=df.groupBy(df.department).min('salary')\n",
    "df4.printSchema()\n",
    "# df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- max(salary): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#maximin salary of each department using max()\n",
    "\n",
    "df5=df.groupBy('department').max('salary')\n",
    "df5.printSchema()\n",
    "# df5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- avg(salary): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the average salary of each department using avg()\n",
    "\n",
    "df6=df.groupBy('department').avg('salary')\n",
    "df6.printSchema()\n",
    "# df6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- avg(salary): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the mean salary of each department using mean()\n",
    "df7=df.groupBy('department').mean('salary')\n",
    "df7.printSchema()\n",
    "# df6.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also run groupBy and aggregate on two or more DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- sum(salary): long (nullable = true)\n",
      " |-- sum(bonus): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group by on department,state and does sum() on salary and bonus columns.\n",
    "df7=df.groupBy('department','state').sum('salary','bonus')\n",
    "df7.printSchema()\n",
    "# df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running more aggregates at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using agg() aggregate function we can calculate many aggregations at a time on a single statement using SQL functions sum(), avg(), min(), max() mean() e.t.c. In order to use these, we should import \"from pyspark.sql.functions import sum,avg,max,min,mean,count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- count(1): long (nullable = false)\n",
      " |-- sum(salary): long (nullable = true)\n",
      " |-- avg(salary): double (nullable = true)\n",
      " |-- sum(bonus): long (nullable = true)\n",
      " |-- max(bonus): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "df9=df.groupBy(\"department\") \\\n",
    "    .agg(count('*'),sum(\"salary\"), \\\n",
    "         avg(\"salary\"), \\\n",
    "         sum(\"bonus\"), \\\n",
    "         max(\"bonus\")\\\n",
    "     ) \n",
    "df9.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- count all: long (nullable = false)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- sum_bonus: long (nullable = true)\n",
      " |-- max_bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "df9=df.groupBy(\"department\") \\\n",
    "    .agg(count('*').alias('count all'),sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \n",
    "df9.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using filter on aggregate data  # where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: string (nullable = true)\n",
      " |-- sum_salary: long (nullable = true)\n",
      " |-- avg_salary: double (nullable = true)\n",
      " |-- sum_bonus: long (nullable = true)\n",
      " |-- max_bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,col\n",
    "df9=df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000)\n",
    "df9.printSchema() \n",
    "# df9.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
