{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explode(), split(), array() & array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-27E2FOUT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>explode() split() array() and  array_contains()</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x260539e1120>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"explode() split() array() and  array_contains()\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'Maheer',['aws','azure']),(2,'wafa',['Gcp','Alibaba'])]\n",
    "schema=['id','name','skills']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- skill: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "#creating new column with explode values\n",
    "df2=df.withColumn('skill',explode(df.skills))\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split(): - it convert string into array type seperated with delimator like , and ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'Maheer','aws,azure'),(2,'wafa','Gcp,Alibaba')]\n",
    "schema=['id','name','skills']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      " |-- skill: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,split\n",
    "#creating new column with array of sytring types values\n",
    "df2=df.withColumn('skill',split(df.skills,','))#\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array():- used to merge two columns with same data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pSkills: string (nullable = true)\n",
      " |-- sSkills: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pSkills: string (nullable = true)\n",
      " |-- sSkills: string (nullable = true)\n",
      " |-- skill: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'Maheer','aws','azure'),(2,'wafa','Gcp','Alibaba')]\n",
    "schema=['id','name','pSkills','sSkills']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()\n",
    "from pyspark.sql.functions import explode,split,array\n",
    "#creating new column with array of sytring types values\n",
    "df2=df.withColumn('skill',array(df.pSkills,df.sSkills))#\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array_contains():-it will check that the value is present int the given array column or not ,it returns \n",
    "                    Null if array is null or true if the value is present or false if value is not present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(1,'Maheer',['aws','azure']),(2,'wafa',['Gcp','Alibaba'])]\n",
    "schema=['id','name','skills']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- HasAwsSkills: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df3=df.withColumn('HasAwsSkills',array_contains(df.skills,'aws'))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function explode in module pyspark.sql.functions:\n",
      "\n",
      "explode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "    Returns a new row for each element in the given array or map.\n",
      "    Uses the default column name `col` for elements in the array and\n",
      "    `key` and `value` for elements in the map unless specified otherwise.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    col : :class:`~pyspark.sql.Column` or str\n",
      "        target column to work on.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        one row per array item or map key value.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    :meth:`pyspark.functions.posexplode`\n",
      "    :meth:`pyspark.functions.explode_outer`\n",
      "    :meth:`pyspark.functions.posexplode_outer`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "    >>> df.select(explode(df.intlist).alias(\"anInt\")).collect()\n",
      "    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "    \n",
      "    >>> df.select(explode(df.mapfield).alias(\"key\", \"value\")).show()\n",
      "    +---+-----+\n",
      "    |key|value|\n",
      "    +---+-----+\n",
      "    |  a|    b|\n",
      "    +---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split in module pyspark.sql.functions:\n",
      "\n",
      "split(str: 'ColumnOrName', pattern: str, limit: int = -1) -> pyspark.sql.column.Column\n",
      "    Splits str around matches of the given pattern.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    str : :class:`~pyspark.sql.Column` or str\n",
      "        a string expression to split\n",
      "    pattern : str\n",
      "        a string representing a regular expression. The regex string should be\n",
      "        a Java regular expression.\n",
      "    limit : int, optional\n",
      "        an integer which controls the number of times `pattern` is applied.\n",
      "    \n",
      "        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                         resulting array's last entry will contain all input beyond the last\n",
      "                         matched pattern.\n",
      "        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                          array can be of any size.\n",
      "    \n",
      "        .. versionchanged:: 3.0\n",
      "           `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        array of separated strings.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "    [Row(s=['one', 'twoBthreeC'])]\n",
      "    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "    [Row(s=['one', 'two', 'three', ''])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array in module pyspark.sql.functions:\n",
      "\n",
      "array(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "    Creates a new array column.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : :class:`~pyspark.sql.Column` or str\n",
      "        column names or :class:`~pyspark.sql.Column`\\s that have\n",
      "        the same data type.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        a column of array type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "    >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "    [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "    >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "    [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "    >>> df.select(array('age', 'age').alias(\"col\")).printSchema()\n",
      "    root\n",
      "     |-- col: array (nullable = false)\n",
      "     |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array_contains in module pyspark.sql.functions:\n",
      "\n",
      "array_contains(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "    Collection function: returns null if the array is null, true if the array contains the\n",
      "    given value, and false otherwise.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    col : :class:`~pyspark.sql.Column` or str\n",
      "        name of column containing array\n",
      "    value :\n",
      "        value or column to check for in array\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        a column of Boolean type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "    >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(array_contains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
