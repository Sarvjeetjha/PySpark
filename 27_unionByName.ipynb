{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-27E2FOUT:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>unionByName </code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x242106c7df0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('unionByName ').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unionByName():-unionByName() is used to merge or union two DataFrames with different column names and a different number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between PySpark uionByName() vs union():-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between unionByName() function and union() is that this function\n",
    "resolves columns by name (not by position). In other words, unionByName() is used to merge two DataFrames by column names instead of by position.\n",
    "\n",
    "unionByName() also provides an argument allowMissingColumns to specify if you have a different column counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",34), (\"Michael\",56), (\"Robert\",30), (\"Maria\",24) ]\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
    "df1.printSchema()\n",
    "\n",
    "# Create DataFrame df2 with columns id and name\n",
    "data2=[(34,\"James\"),(45,\"Maria\"), (45,\"Jen\"),(34,\"Jeff\")]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PySpark unionByName() is used to union two DataFrames when you have column names in a different order or even if you have missing columns in any DataFrme, in other words, this function\n",
    "resolves columns by name (not by position). \"\"\"\n",
    "df3=df1.unionByName(df2)\n",
    "df3.printSchema()\n",
    "# df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unionByName() with Different Number of Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you have a different number of columns then use allowMissingColumns=True. When using this, the result of the DataFrmae contains null values for the columns that are missing on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col0: long (nullable = true)\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with different column names\n",
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 3]], [\"col1\",  \"col3\"])\n",
    "\n",
    "# Using allowMissingColumns\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema()\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " select():-select() function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Single & Multiple Columns\n",
    "df.select(\"firstname\",\"lastname\").show()\n",
    "df.select(df.firstname,df.lastname).show()\n",
    "df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
    "\n",
    "#By using col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
    "\n",
    "#Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
